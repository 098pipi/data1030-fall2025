{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudcard\n",
    "- **what is purpose of the number in the random_state, or is that not really important?**\n",
    "    - It is extremely important to use a number, it's not really important what number you use\n",
    "    - Rerun the cell a couple of times when the random_state argument is removed and check which points are in the training set\n",
    "    - Then fix the random state to be some number, and rerun the cells again.\n",
    "    - Then use another number as the random state, and rerun the cells again.\n",
    "- **Why do we use the same test data in final evaluation.**\n",
    "    - I assume this is fot k-fold splitting\n",
    "    - You need to use the test set only once, after you are done with cross-validation\n",
    "    - Having said that, when you change the random state in train_test_split (and you will be required to do so), different points will be in the test set each time\n",
    "- **Is there a more systematic alternative to random shuffling that ensures even representation of all classes? What if the dataset is imbalanced?**\n",
    "    - Yes, you can do a stratified split, we will talk about this during the second half of the term.\n",
    "- **Also, is it possible to ensure all \"types of feature matrices\" are well-represented in all sets?**\n",
    "    - Usually that's not a requirement. You want to make sure the target variable is evenly represented.\n",
    "- **I'm still sort of confused what the difference is between validation and testing sets, and why both are needed**\n",
    "    - I hope all of this will be clear in less than two weeks!\n",
    "- **I would like to go over the parameters of the train test split method. I understand conceptually be would like to have a breakdown of how the use the function.**\n",
    "    - Write some test code and experiment with all the arguments. I only have time to discuss what I think are the most important arguments in class.\n",
    "- **Once you train a model with your training data, then validate and test it, do you then make a 'final' model trained on all the data?**\n",
    "    - You can retrain the model on X_other and y_other.\n",
    "    - You usually don't use X_test and y_test when you retrain the model.\n",
    "- **Why does k-fold without shuffling exist if it makes an iid dataset less random?**\n",
    "    - Because non-iid datasets also exist and for those, it makes sense to not shuffle sometimes\n",
    "- **What does it mean to set aside one feature for classifying and use the other categories as info for the model?**\n",
    "    - I'm not sure what you are refering to. Please post on the course forum or talk to me during my office hours.\n",
    "- **I was a bit confused about why we need to do two train-test splits.**\n",
    "    - Because we want three sets: train, validation, and test\n",
    "    - Train_test_split only splits a dataset into two part, not three.\n",
    "    - So it needs to be applied twice.\n",
    "- **The 0.75 value specifically**\n",
    "- **Can we review how we calculate the split for the training set? (Quiz 2 answer)**\n",
    "- **I am confused about the fractions aspect of this. Is there a resource to study that?**\n",
    "    - this is high school math so I don't really have good recommendations.\n",
    "    - Read through the quiz again carefully to work out the fractions.\n",
    "    - Come to the office hours if you need help.\n",
    "- **\"When should we prefer K-Fold cross-validation over a simple train/validation/test split?**\n",
    "    - train/val/test split is usually used for large datasets because you only need to train one model per hyperparameter.\n",
    "    - kfold is better suited for small to medium datasets when you care less about computational efficiency.\n",
    "    - in kfold, you'll train k number of models for each hyperparameter\n",
    "- **How the KFold object works?**\n",
    "    - work with the code provided in the lecture notes to figure it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Lecture 6: Data preprocessing</center>\n",
    "\n",
    "### By the end of this lecture, you will be able to\n",
    "- apply one-hot encoding on categorical features\n",
    "- apply ordinal encoding on ordinal features\n",
    "- apply scaling and normalization to continuous variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The supervised ML pipeline\n",
    "\n",
    "**0. Data collection/manipulation**: you might have multiple data sources and/or you might have more data than you need\n",
    "   - you need to be able to read in datasets from various sources (like csv, excel, SQL, parquet, etc)\n",
    "   - you need to be able to filter the columns/rows you need for your ML model\n",
    "   - you need to be able to combine the datasets into one dataframe \n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "    \n",
    "**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "<span style=\"background-color: #FFFF00\">**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)</span>\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to be transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "    \n",
    "**4. Choose an evaluation metric**: depends on the priorities of the stakeholders\n",
    "   - often requires quite a bit of thinking and ethical considerations\n",
    "     \n",
    "**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "    \n",
    "**6. Tune the hyperparameters of your ML models (aka cross-validation or hyperparameter tuning)**\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem description, why preprocessing is necessary\n",
    "\n",
    "Data format suitable for ML: 2D numerical values.\n",
    "\n",
    "| X|feature_1|feature_2|...|feature_j|...|feature_m|<font color='red'>y</font>|\n",
    "|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__data_point_1__|x_11|x_12|...|x_1j|...|x_1m|__<font color='red'>y_1</font>__|\n",
    "|__data_point_2__|x_21|x_22|...|x_2j|...|x_2m|__<font color='red'>y_2</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>__|\n",
    "|__data_point_i__|x_i1|x_i2|...|x_ij|...|x_im|__<font color='red'>y_i</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>__|\n",
    "|__data_point_n__|x_n1|x_n2|...|x_nj|...|x_nm|__<font color='red'>y_n</font>__|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data almost never comes in a format that's directly usable in ML.\n",
    "- let's check the adult data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "df = pd.read_csv('../data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# first split to separate out the training set\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=random_state)\n",
    "\n",
    "# second split to separate out the validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=random_state)\n",
    "\n",
    "print('training set')\n",
    "print(X_train.head()) # lots of strings!\n",
    "print(y_train.head()) # even our labels are strings and not numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### scikit-learn transformers to the rescue!\n",
    "\n",
    "Preprocessing is done with various transformers. All transformes have three methods:\n",
    "- **fit** method: estimates parameters necessary to do the transformation,\n",
    "- **transform** method: transforms the data based on the estimated parameters,\n",
    "- **fit_transform** method: both steps are performed at once, this can be faster than doing the steps separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformers we cover today\n",
    "- **OneHotEncoder** - converts categorical features into dummy arrays\n",
    "- **OrdinalEncoder** - converts ordinal features into an integer array\n",
    "- **MinMaxScaler** - scales continuous variables to be between 0 and 1\n",
    "- **StandardScaler** - standardizes continuous features by removing the mean and scaling to unit variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- **apply one-hot encoding on categorical features**\n",
    "- <font color='LIGHTGRAY'>apply ordinal encoding on ordinal features</font>\n",
    "- <font color='LIGHTGRAY'>apply scaling and normalization to continuous variables</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unordered categorical data: one-hot encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- some categories cannot be ordered. e.g., workclass, relationship status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "help(OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example\n",
    "train = {'gender':['Male','Female','Unknown','Male','Female','Female'],\\\n",
    "         'browser':['Safari','Safari','Internet Explorer','Chrome','Chrome','Internet Explorer']}\n",
    "test = {'gender':['Female','Male','Unknown','Female'],'browser':['Chrome','Firefox','Internet Explorer','Safari']}\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train)\n",
    "Xtoy_test = pd.DataFrame(test)\n",
    "\n",
    "ftrs = ['gender','browser']\n",
    "\n",
    "# initialize the encoder\n",
    "enc = OneHotEncoder(sparse_output=False) # handle_unknown='ignore'  # by default, OneHotEncoder returns a sparse matrix. sparse_output=False returns a 2D array\n",
    "# fit the training data\n",
    "enc.fit(Xtoy_train)\n",
    "print('categories:',enc.categories_)\n",
    "print('feature names:',enc.get_feature_names_out(ftrs))\n",
    "# transform X_train\n",
    "X_train_ohe = enc.transform(Xtoy_train)\n",
    "#print(X_train_ohe)\n",
    "# do all of this in one step\n",
    "X_train_ohe = enc.fit_transform(Xtoy_train)\n",
    "print('X_train transformed')\n",
    "print(X_train_ohe)\n",
    "\n",
    "# transform X_test\n",
    "X_test_ohe = enc.transform(Xtoy_test)\n",
    "print('X_test transformed')\n",
    "print(X_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# apply OHE to the adult dataset\n",
    "\n",
    "# let's collect all categorical features first\n",
    "onehot_ftrs = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "# initialize the encoder\n",
    "enc = OneHotEncoder(sparse_output=False,handle_unknown='ignore') # by default, OneHotEncoder returns a sparse matrix. sparse_output=False returns a 2D array\n",
    "# fit the training data\n",
    "enc.fit(X_train[onehot_ftrs])\n",
    "print('feature names:',enc.get_feature_names_out(onehot_ftrs))\n",
    "print(len(enc.get_feature_names_out(onehot_ftrs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# transform X_train\n",
    "onehot_train = enc.transform(X_train[onehot_ftrs])\n",
    "print('transformed train features:')\n",
    "print(onehot_train)\n",
    "# transform X_val\n",
    "onehot_val = enc.transform(X_val[onehot_ftrs])\n",
    "print('transformed val features:')\n",
    "print(onehot_val)\n",
    "# transform X_test\n",
    "onehot_test = enc.transform(X_test[onehot_ftrs])\n",
    "print('transformed test features:')\n",
    "print(onehot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>apply one-hot encoding on categorical features</font>\n",
    "- **apply ordinal encoding on ordinal features**\n",
    "- <font color='LIGHTGRAY'>apply scaling and normalization to continuous variables</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ordered categorical data: OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use it on categorical features if the categories can be ranked or ordered\n",
    "    - educational level in the adult dataset\n",
    "    - reaction to medication is described by words like 'severe', 'no response', 'excellent'\n",
    "    - any time you know that the categories can be clearly ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "help(OrdinalEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example\n",
    "import pandas as pd\n",
    "\n",
    "train_edu = {'educational level':['Bachelors','Masters','Bachelors','Doctorate','HS-grad','Masters']} \n",
    "test_edu = {'educational level':['HS-grad','Masters','Masters','College','Bachelors']}\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train_edu)\n",
    "Xtoy_test = pd.DataFrame(test_edu)\n",
    "\n",
    "# initialize the encoder\n",
    "cats = [['HS-grad','College','Bachelors','Masters','Doctorate']]\n",
    "\n",
    "enc = OrdinalEncoder(categories = cats) # The ordered list of \n",
    "# categories need to be provided. By default, the categories are alphabetically ordered!\n",
    "\n",
    "# fit the training data\n",
    "enc.fit(Xtoy_train)\n",
    "# print the categories - not really important because we manually gave the ordered list of categories\n",
    "print(enc.categories_)\n",
    "# transform X_train. We could have used enc.fit_transform(X_train) to combine fit and transform\n",
    "X_train_oe = enc.transform(Xtoy_train)\n",
    "print(X_train_oe)\n",
    "# transform X_test\n",
    "X_test_oe = enc.transform(Xtoy_test) # OrdinalEncoder always throws an error message if \n",
    "                                  # it encounters an unknown category in test\n",
    "print(X_test_oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# apply OE to the adult dataset\n",
    "# initialize the encoder\n",
    "ordinal_ftrs = ['education'] # if you have more than one ordinal feature, add the feature names here\n",
    "ordinal_cats = [[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th',' HS-grad',\\\n",
    "                ' Some-college',' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',' Prof-school',' Doctorate']]\n",
    "# ordinal_cats must contain one list per ordinal feature! each list contains the ordered list of categories \n",
    "# of the corresponding feature\n",
    "\n",
    "enc = OrdinalEncoder(categories = ordinal_cats)   # By default, the categories are alphabetically ordered\n",
    "                                                    # which is NOT what you want usually.\n",
    "\n",
    "# fit the training data\n",
    "enc.fit(X_train[ordinal_ftrs])  # the encoder expects a 2D array, that's why the column name is in a list\n",
    "\n",
    "# transform X_train. We could use enc.fit_transform(X_train) to combine fit and transform\n",
    "ordinal_train = enc.transform(X_train[ordinal_ftrs])\n",
    "print('transformed train features:')\n",
    "print(ordinal_train)\n",
    "# transform X_val\n",
    "ordinal_val = enc.transform(X_val[ordinal_ftrs])\n",
    "print('transformed validation features:')\n",
    "print(ordinal_val)\n",
    "# transform X_test\n",
    "ordinal_test = enc.transform(X_test[ordinal_ftrs])\n",
    "print('transformed test features:')\n",
    "print(ordinal_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz 1\n",
    "Please explain how you would encode the race feature below and what would be the output of the encoder. Do not write code. The goal of this quiz is to test your conceptual understanding so write text and the output array.\n",
    "\n",
    "race = [' Amer-Indian-Eskimo', 'White', 'Black', 'Asian-Pac-Islander', 'Black', 'White', 'White']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>apply one-hot encoding on categorical features</font>\n",
    "- <font color='LIGHTGRAY'>apply ordinal encoding on ordinal features</font>\n",
    "- **apply scaling and normalization to continuous variables**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous features: MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the continuous feature values are reasonably bounded, MinMaxScaler is a good way to scale the features.\n",
    "- Age is expected to be within the range of 0 and 100.\n",
    "- Number of hours worked per week is in the range of 0 to 80.\n",
    "- If unsure, plot the histogram of the feature to verify or just go with the standard scaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "help(MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy data\n",
    "# let's assume we have two continuous features:\n",
    "train = {'age':[32,65,13,68,42,75,32],'number of hours worked':[0,40,10,60,40,20,40]}\n",
    "test = {'age':[83,26,10,60],'number of hours worked':[0,40,0,60]}\n",
    "\n",
    "# (value - min) / (max - min), if value is 32, min is 13 and max is 75, then we have 19 / 62 = 0.3064\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train)\n",
    "Xtoy_test = pd.DataFrame(test)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(Xtoy_train)\n",
    "print(scaler.transform(Xtoy_train))\n",
    "print(scaler.transform(Xtoy_test)) # note how scaled X_test contains values larger than 1 and smaller than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# adult data\n",
    "\n",
    "minmax_ftrs = ['age','hours-per-week']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train[minmax_ftrs])\n",
    "print(scaler.transform(X_train[minmax_ftrs]))\n",
    "print(scaler.transform(X_val[minmax_ftrs])) \n",
    "print(scaler.transform(X_test[minmax_ftrs])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous features: StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the continuous feature values follow a tailed distribution, StandardScaler is better to use!\n",
    "- Salaries are a good example. Most people earn less than 100k but there are a small number of super-rich people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "help(StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy data\n",
    "train = {'salary':[50_000,75_000,40_000,1_000_000,30_000,250_000,35_000,45_000]}\n",
    "test = {'salary':[25_000,55_000,1_500_000,60_000]}\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train)\n",
    "Xtoy_test = pd.DataFrame(test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit_transform(Xtoy_train))\n",
    "print(scaler.transform(Xtoy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# adult data\n",
    "\n",
    "std_ftrs = ['capital-gain','capital-loss']\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit_transform(X_train[std_ftrs]))\n",
    "print(scaler.transform(X_val[std_ftrs]))\n",
    "print(scaler.transform(X_test[std_ftrs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz 2\n",
    "\n",
    "Which of these features could be safely preprocessed by the minmax scaler?\n",
    "- number of minutes spent on the website in a day\n",
    "- number of days a year spent abroad in a year\n",
    "- USD donated to charity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How and when to do preprocessing in the ML pipeline?\n",
    "\n",
    "- **APPLY TRANSFORMER.FIT ONLY ON YOUR TRAINING DATA!** Then transform the validation and test sets.\n",
    "- One of the most common mistake practitioners make is leaking statistics!\n",
    "     - fit_transform is applied to the whole dataset, then the data is split into train/validation/test\n",
    "         - this is wrong because the test set statistics impacts how the training and validation sets are transformed\n",
    "         - but the test set must be separated from train and val, and val must be separated from train\n",
    "     - or fit_transform is applied to the train, then fit_transform is applied to the validation set, and fit_transform is applied to the test set\n",
    "         - this is wrong because the relative position of the points change\n",
    "<center><img src=\"../figures/no_separate_scaling.png\" width=\"1200\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scikit-learn's pipelines\n",
    "\n",
    "- The steps in the ML pipleine can be chained together into a scikit-learn pipeline which consists of transformers and one final estimator which is usually your classifier or regression model.\n",
    "- It neatly combines the preprocessing steps and it helps to avoid leaking statistics.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#np.random.seed(0)\n",
    "\n",
    "df = pd.read_csv('../data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# first split to separate out the training set\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=random_state)\n",
    "\n",
    "# second split to separate out the validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect which encoder to use on each feature\n",
    "# needs to be done manually\n",
    "ordinal_ftrs = ['education'] \n",
    "ordinal_cats = [[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th',' HS-grad',\\\n",
    "                ' Some-college',' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',' Prof-school',' Doctorate']]\n",
    "onehot_ftrs = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "minmax_ftrs = ['age','hours-per-week']\n",
    "std_ftrs = ['capital-gain','capital-loss']\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse_output=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess \n",
    "                                                       # later on we will add other steps here\n",
    "\n",
    "X_train_prep = clf.fit_transform(X_train)\n",
    "X_val_prep = clf.transform(X_val)\n",
    "X_test_prep = clf.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train_prep.shape)\n",
    "print(X_train_prep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
